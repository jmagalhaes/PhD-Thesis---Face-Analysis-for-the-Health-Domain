%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% abstrac-en.tex
%% UNL thesis document file
%%
%% Abstract in English
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Communication is a crucial part of our daily lives. Through communication we exchange ideas, opinions, feelings, and much more.
However, neurological disorders or traumatic brain injuries can affect speech production through abnormalities in speech motor control.
Besides causing difficulties in speech production, neurological disorders can also cause difficulty in expressing emotions. Patients with
\gls{pd}, e.g., develop difficulties in speech production and in maintaining voice quality, but also in emotional facial
expressiveness.
Even though the expression of emotions is not directly affected by an existing neurological problem, speech disabilities can cause
frustration and reduced self-esteem which on the other hand can enhance symptoms of speech disorders. A prime example concerning this matter is \gls{stuttering}.

As human communication involves the complex processing of different sources of information, this PhD thesis aims to develop a multimodal machine learning framework able to relate characteristics of motor speech disorders such as, \gls{dysarthria}, \gls{apraxia}, or \gls{stuttering}, with expressed emotions. For that purpose, state-of-the-art techniques used in video, audio, and physiological data analysis will be explored in order to develop a multimodal representation which captures correspondences between the modalities. 

\begin{comment}
To understand the relationship between motor speech disorders and expressed emotions, the goal of this Phd thesis is to collect data of patients having Dysarthria, Apraxia of speech or Stuttering. As communication by itself is multimodal, the data will be collected using different modalities to gather video, audio and physiological data.
The data collected will be labeled by Speech Language Pathologists (SLPs) and analyzed using hand-crafted features. The data of the
different modalities will be analyzed separately using different types of classification (binary, multi-class, multi-label), as well as fused
together in order to evaluate if the different modalities bring more insights if they are joined in the feature space prior to classification.
\end{comment}

By developing a multimodal framework able to detect expressions of emotions (affect) as well as speech errors in different modalities, the hope is to confirm mathematically the relationship between the severity of motor speech disorders and emotional expressiveness. This could to a new measure of severity of neurodegenerative disorders and bring new insights to therapy design, e.g., provide patients with biofeedback, such that they can adapt their emotional state to one that supports speech production instead of
sabotaging it. 



\begin{comment}
By understanding the relationship between the severity of motor speech disorders and the expressed emotions of an individual, the
hope is to provide caregivers insights of the inner emotional state of the patient which otherwise would be difficult to perceive, and to
provide patients with biofeedback, such that they can adapt their emotional state to one that supports speech production instead of
sabotaging it.
\end{comment}

% Palavras-chave do resumo em InglÃªs
\begin{keywords}
facial expressions, motor speech disorders, stuttering, dysarthria, apraxia of speech, emotional expressiveness, multimodal machine learning 
\end{keywords} 



%=================
\begin{comment}
The abstract should not exceed one page and should answer the following questions:

\begin{itemize}
	\item What's the problem?
	\item Why is it interesting?
	\item What's the solution?
	\item What follows from the solution?
\end{itemize}
\end{comment}