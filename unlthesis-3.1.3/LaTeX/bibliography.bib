% Encoding: UTF-8
% INTRO


@Misc{FaceMuscles,
	author = {{Oustormcrowd}},
	title = "Human Face Muscle Anatomy",
	text = {[Online; accessed March 13, 2017]},
	year = "2015",
	url = "http://www.oustormcrowd.com/the-function-of-human-muscle-anatomy-face/anatomy-of-human-face-and-neck-muscles-digital-art/"
}

@Book{PhonManual,
  title     = {Dissection of the Speech Production Mechanism},
  publisher = {UCLA Working Papers in Phonetics 102},
  year      = {2002},
  author    = {The UCLA Phonetics Laboratory},
  editor    = {Melissa Epstein, Narineh Hacopian and Peter Ladefoged}
}

@Book{Burgoon2016,
  title     = {Nonverbal communication},
  publisher = {Routledge},
  year      = {2016},
  author    = {Burgoon, Judee K and Guerrero, Laura K and Floyd, Kory}
}

@Misc{SpeechStatistics,
	author = {{National Institute on Deafness and Other Communication Disorders}},
	title = "Quick Statistics About Voice, Speech, Language",
	text = {[Online; accessed May 13, 2017]},
	year = "2016",
	url = "https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language"
}

@Article{Ruben2000,
  author    = {Ruben, Robert J},
  title     = {Redefining the survival of the fittest: communication disorders in the 21st century},
  journal   = {The Laryngoscope},
  year      = {2000},
  volume    = {110},
  number    = {2},
  pages     = {241--241},
  file      = {:Speech Therapy/Ruben2000.pdf:PDF},
  publisher = {Wiley Online Library}
}


%FACIAL PARALYSIS
@InProceedings{Ngo2016,
  author    = {T. H. Ngo and M. Seo and N. Matsushiro and W. Xiong and Y. W. Chen},
  title     = {Quantitative analysis of facial paralysis based on limited-orientation modified circular Gabor filters},
  booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
  year      = {2016},
  pages     = {349-354},
  month     = {12},
  doi       = {10.1109/ICPR.2016.7899658}
}

@inproceedings{Dong2008,
  title={An approach for quantitative evaluation of the degree of facial paralysis based on salient point detection},
  author={Dong, Junyu and Ma, Lijing and Li, Qingqiang and Wang, Shengke and Liu, Li-an and Lin, Yang and Jian, Muwei},
  booktitle={Intelligent Information Technology Application Workshops, 2008. IITAW'08. International Symposium on},
  pages={483--486},
  year={2008},
  organization={IEEE}
}

@InProceedings{Sundaraj2012,
  author    = {W. S. W. Samsudin and K. Sundaraj},
  title     = {Image processing on facial paralysis for facial rehabilitation system: A review},
  booktitle = {2012 IEEE International Conference on Control System, Computing and Engineering},
  year      = {2012},
  pages     = {259-263},
  month     = {Nov},
  doi       = {10.1109/ICCSCE.2012.6487152},
}


%Affective Computing
@article{Picard2014promise,
  title={The Promise of Affective Computing},
  author={Picard, Rosalind W},
  journal={The Oxford Handbook of Affective Computing},
  volume={11},
  year={2014},
  publisher={Oxford University Press, USA}
}

@Article{Corneanu2016survey,
  author    = {Corneanu, Ciprian Adrian and Sim{\'o}n, Marc Oliu and Cohn, Jeffrey F and Guerrero, Sergio Escalera},
  title     = {Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2016},
  volume    = {38},
  number    = {8},
  pages     = {1548--1568},
  file      = {:FacialExpressions/Corneanu2016survey.pdf:PDF},
  groups    = {Facial Expressions},
  publisher = {IEEE},
}


% FUNDAMENTALS

@Manual{SLPathologies,
  title        = {Speech-Language Pathology Medical Review Guidelines},
  organization = {American Speech-Language-Hearing Association},
  year         = {2015},
  note         = {http://www.asha.org/uploadedFiles/SLP-Medical-Review-Guidelines.pdf},
  file         = {:Speech Therapy/SLP-Medical-Review-Guidelines.pdf:PDF}
}

@Misc{communicationDifficulties,
  author = {{icommunicate therapy}},
  title  = {Progressive Neurological Diseases and Communication Difficulties},
  year   = {2017},
  file   = {:Speech Therapy/communicationDifficulties.pdf:PDF},
  groups = {Speech},
  text   = {[Online; accessed May 31, 2017]},
  url    = {http://www.icommunicatetherapy.com/adult-communication-difficulties-2/adult-acquired-communication-difficulties/progressive-neurological-diseases/},
}

@InCollection{Prendergast2013anatomy,
  author    = {Prendergast, Peter M},
  title     = {Anatomy of the face and neck},
  booktitle = {Cosmetic Surgery},
  publisher = {Springer},
  year      = {2013},
  pages     = {29--45},
  file      = {:Speech Therapy/Prendergast2013.pdf:PDF},
}


@Article{Poria2017,
  author   = {Soujanya Poria and Erik Cambria and Rajiv Bajpai and Amir Hussain},
  title    = {A review of affective computing: From unimodal analysis to multimodal fusion},
  journal  = {Information Fusion},
  year     = {2017},
  volume   = {37},
  pages    = {98 - 125},
  doi      = {http://dx.doi.org/10.1016/j.inffus.2017.02.003},
  file     = {:FacialExpressions/Poria2017.pdf:PDF},
  groups   = {Survey, Facial Expressions},
  issn     = {1566-2535},
  keywords = {Affective computing, Sentiment analysis, Multimodal affect analysis, Multimodal fusion, Audio, visual and text information fusion },
  url      = {http://www.sciencedirect.com/science/article/pii/S1566253517300738},
}

@Article{Kruse1997,
  author  = {Kruse, Douglas},
  title   = {Employment and disability: Characteristics of employed and non-employed people with disabilities},
  journal = {A Report to the US Department of Labor, Final Report (September)},
  year    = {1997},
  groups  = {Speech},
}






@Article{Baltruvsaitis2017multimodal,
  author  = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  title   = {Multimodal Machine Learning: A Survey and Taxonomy},
  journal = {arXiv preprint arXiv:1705.09406},
  year    = {2017},
  file    = {:Multimodal Fusion/Tadas2017.pdf:PDF},
}

@inproceedings{Ngiam2011,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={689--696},
  year={2011}
}

@Article{Narayanan2013,
  author        = {Narayanan, Shrikanth and Georgiou, Panayiotis G},
  title         = {Behavioral signal processing: Deriving human behavioral informatics from speech and language},
  journal       = {Proceedings of the IEEE},
  year          = {2013},
  volume        = {101},
  number        = {5},
  pages         = {1203--1233},
  file          = {:Speech+Emotion/Narayanan2013.pdf:PDF},
  groups        = {Multimodal},
  publisher     = {IEEE},
}

@Article{Dmello2015review,
  author    = {D'mello, Sidney K and Kory, Jacqueline},
  title     = {A review and meta-analysis of multimodal affect detection systems},
  journal   = {ACM Computing Surveys (CSUR)},
  year      = {2015},
  volume    = {47},
  number    = {3},
  pages     = {43},
  file      = {:Multimodal Fusion/Dmello2015.pdf:PDF},
  groups    = {Multimodal},
  publisher = {ACM},
}

@Article{Bengio2013representation,
  author        = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title         = {Representation learning: A review and new perspectives},
  journal       = {IEEE transactions on pattern analysis and machine intelligence},
  year          = {2013},
  volume        = {35},
  number        = {8},
  pages         = {1798--1828},
  file          = {:Multimodal Fusion/Bengio2013.pdf:PDF},
  groups        = {Multimodal, To Read},
  keywords      = {rank5},
  publisher     = {IEEE},
}

@InProceedings{Srivastava2012,
  author    = {Srivastava, Nitish and Salakhutdinov, Ruslan R},
  title     = {Multimodal learning with deep boltzmann machines},
  booktitle = {Advances in neural information processing systems},
  year      = {2012},
  pages     = {2222--2230},
  file      = {:Multimodal Fusion/Srivastava2012.pdf:PDF},
  groups    = {To Read, Multimodal},
}
@Article{Baumann2017,
  author   = {Baumann, Nicole and Palasik, Dr and others},
  title    = {The Effects of Music Therapy on Stuttering},
  year     = {2017},
  file     = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Baumann2017.pdf:PDF},
  keywords = {stuttering, music, emotion},
}

@Article{Boyle2016,
  author    = {Boyle, Michael P and Dioguardi, Lauren and Pate, Julie E},
  title     = {A comparison of three strategies for reducing the public stigma associated with stuttering},
  journal   = {Journal of fluency disorders},
  year      = {2016},
  volume    = {50},
  pages     = {44--58},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Boyle2016.pdf:PDF},
  keywords  = {stuttering, society},
  publisher = {Elsevier},
}

@Article{Choi2016,
  author    = {Choi, Dahye and Conture, Edward G and Walden, Tedra A and Jones, Robin M and Kim, Hanjoe},
  title     = {Emotional diathesis, emotional stress, and childhood stuttering},
  journal   = {Journal of Speech, Language, and Hearing Research},
  year      = {2016},
  volume    = {59},
  number    = {4},
  pages     = {616--630},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Choi2015.pdf:PDF},
  keywords  = {stuttering, emotional Speech},
  publisher = {ASHA},
}

@InProceedings{Chee2009,
  author       = {Chee, Lim Sin and Ai, Ooi Chia and Hariharan, M and Yaacob, Sazali},
  title        = {MFCC based recognition of repetitions and prolongations in stuttered speech using k-NN and LDA},
  booktitle    = {Research and Development (SCOReD), 2009 IEEE Student Conference on},
  year         = {2009},
  pages        = {146--149},
  organization = {IEEE},
  file         = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Chee2009.pdf:PDF},
  keywords     = {Stuttering, MFCC, knn, LDA},
}

@Article{Freud2017,
  author    = {Freud, Debora and Kichin-Brin, Marina and Ezrati-Vinacour, Ruth and Roziner, Ilan and Amir, Ofer},
  title     = {The relationship between the experience of stuttering and demographic characteristics of adults who stutter},
  journal   = {Journal of Fluency Disorders},
  year      = {2017},
  volume    = {52},
  pages     = {53--63},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Freud2017.pdf:PDF},
  keywords  = {Stuttering},
  publisher = {Elsevier},
}

@Article{Harasym2015,
  author    = {Harasym, Jessica and Langevin, Marilyn and Kully, Deborah},
  title     = {Video self-modeling as a post-treatment fluency recovery strategy for adults},
  journal   = {Journal of fluency disorders},
  year      = {2015},
  volume    = {44},
  pages     = {32--45},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Harasym2015.pdf:PDF},
  keywords  = {stuttering, video, treatment},
  publisher = {Elsevier},
}

@InProceedings{Hariharan2012,
  author       = {Hariharan, M and Vijean, Vikneswaran and Fook, CY and Yaacob, Sazali},
  title        = {Speech stuttering assessment using sample entropy and Least Square Support Vector Machine},
  booktitle    = {Signal Processing and its Applications (CSPA), 2012 IEEE 8th International Colloquium on},
  year         = {2012},
  pages        = {240--245},
  organization = {IEEE},
  file         = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Hariharan2012.pdf:PDF},
  keywords     = {Stuttering, SVM, entropy},
}

@Article{Ingham2017,
  author    = {Ingham, Roger J and Ingham, Janis C and Euler, Harald A and Neumann, Katrin},
  title     = {Stuttering treatment and brain research in adults: a still unfolding relationship},
  journal   = {Journal of Fluency Disorders},
  year      = {2017},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Ingham2017.pdf:PDF},
  keywords  = {stuttering, brain imaging,},
  publisher = {Elsevier},
}

@InProceedings{jhawar2016speech,
  author       = {Jhawar, Gunjan and Nagraj, Prajacta and Mahalakshmi, P},
  title        = {Speech disorder recognition using MFCC},
  booktitle    = {Communication and Signal Processing (ICCSP), 2016 International Conference on},
  year         = {2016},
  pages        = {0246--0250},
  organization = {IEEE},
  file         = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Jhawar2016.pdf:PDF},
  keywords     = {Stuttering, MFCC},
}

@Article{Jiang2012,
  author    = {Jiang, Jing and Lu, Chunming and Peng, Danling and Zhu, Chaozhe and Howell, Peter},
  title     = {Classification of types of stuttering symptoms based on brain activity},
  journal   = {PloS one},
  year      = {2012},
  volume    = {7},
  number    = {6},
  pages     = {e39747},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Jiang2012.pdf:PDF},
  keywords  = {Stuttering, brain imaging, classification},
  publisher = {Public Library of Science},
}

@Article{Mahesha2016,
  author   = {Mahesha, P and Vinod, DS},
  title    = {Gaussian mixture model based classification of stuttering dysfluencies},
  journal  = {Journal of Intelligent Systems},
  year     = {2016},
  volume   = {25},
  number   = {3},
  pages    = {387--399},
  keywords = {Stuttering, GMM},
}

@Article{Mahesha2015,
  author    = {Mahesha, P and Vinod, DS},
  title     = {Support vector machine-based stuttering dysfluency classification using GMM supervectors},
  journal   = {International Journal of Grid and Utility Computing},
  year      = {2015},
  volume    = {6},
  number    = {3-4},
  pages     = {143--149},
  abstract  = {It is generally acknowledged that recognition and classification of dysfluencies are an important criterion in the objective and accurate assessment of stuttered speech. For this reason, there is a growing interest in the application of Automatic Speech Recognition (ASR) technology to automate the dysfluency recognition. In this perspective, several studies have been carried out on the classification of dysfluencies by means of acoustic analysis, parametric and non-parametric feature extraction and statistical methods. This work is focused on introducing and evaluating Support Vector Machine (SVM) based dysfluency recognition system using a Gaussian Mixture Model (GMM) supervector. The experimental evaluation of the proposed system reveals that an SVM-based GMM supervector is effective for dysfluency classification. We have obtained substantial improvements in the performance by considering cepstral and their delta features.},
  keywords  = {Stuttering, GMM, SVM, speech recognition},
  publisher = {Inderscience Publishers (IEL)},
}

@Article{Pravin2017,
  author   = {Pravin, Sheena Christabel and Anjana, R and Pandiyan, T Prabhu and Ranganath, SK and Rangarajan, Pradeep},
  title    = {ANN Based Disfluent Speech Classification},
  journal  = {Artificial Intelligent Systems and Machine Learning},
  year     = {2017},
  volume   = {9},
  number   = {4},
  pages    = {77--80},
  abstract = {About 1% of theworld population suffer from stuttering, a continued involuntary repetition ofsound; especially initial constants. Another name for this is speech dis-fluency or Disturbed Speech. This includes word repetition, syllable repetition, prolongation, and interjection. The existing algorithm focuses wholly on either extraction or classification. This paper uses "Artificial Neural Network" or ANN and implements automatic analysis of disfluent speech by extracting "Mel frequency cepstral coefficient" or MFCC, Delta MFCC, Delta Delta MFCC and prosodic features like pitch, energy, duration and the like. This paper aims at improving the fluency of the stuttered speech.},
  keywords = {Stuttering, MFCC, ANN},
}

@InProceedings{Ramteke2016,
  author       = {Ramteke, Pravin B and Koolagudi, Shashidhar G and Afroz, Fathima},
  title        = {Repetition Detection in Stuttered Speech},
  booktitle    = {Proceedings of 3rd International Conference on Advanced Computing, Networking and Informatics},
  year         = {2016},
  pages        = {611--617},
  organization = {Springer},
  abstract     = {This paper mainly focuses on detection of repetitions in stuttered speech. The stuttered speech signal is divided into isolated units based on energy. Mel-frequency cepstrum coefficients (MFCCs), formants and shimmer are used as features for repetition recognition. These features are extracted from each isolated unit. Using Dynamic Time Warping (DTW) the features of each isolated unit are compared with those subsequent units within one second interval of speech. Based on the analysis of scores obtained from DTW a threshold is set, if the score is below the set threshold then the units are identified as repeated events. Twenty seven seconds of speech data used in this work, consists of 50 repetition events. The result shows that the combination of MFCCs, formants and shimmer can be used for the recognition of repetitions in stuttered speech. Out of 50 repetitions, 47 are correctly identified.},
  keywords     = {Stuttering, MFCC, Dynamic time warping},
}

@Article{saltuklaroglu2017eeg,
  author    = {Saltuklaroglu, Tim and Harkrider, Ashley W and Thornton, David and Jenson, David and Kittilstved, Tiffani},
  title     = {EEG Mu ($\mu$) rhythm spectra and oscillatory activity differentiate stuttering from non-stuttering adults},
  journal   = {NeuroImage},
  year      = {2017},
  volume    = {153},
  pages     = {232--245},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Saltuklaroglu2017.pdf:PDF},
  keywords  = {EEG, Stuttering,},
  publisher = {Elsevier},
}

@InProceedings{Savin2016,
  author       = {Savin, PS and Ramteke, Pravin B and Koolagudi, Shashidhar G},
  title        = {Recognition of Repetition and Prolongation in Stuttered Speech Using ANN},
  booktitle    = {Proceedings of 3rd International Conference on Advanced Computing, Networking and Informatics},
  year         = {2016},
  pages        = {65--71},
  organization = {Springer},
  keywords     = {stuttering, neural networks},
}

@Article{Smith2017,
  author   = {Smith, Kylie Anne},
  title    = {ANXIETY AND STUTTERING: CLOSING THE RESEARCH GAPS},
  year     = {2017},
  file     = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Smith2017.pdf:PDF},
  keywords = {stuttering, anxiety, emotional speech},
}

@Article{Vanryckeghem2001,
  author   = {Martine Vanryckeghem and Carl Hylebos and Gene J Brutten and Martin Peleman},
  title    = {The relationship between communication attitude and emotion of children who stutter},
  journal  = {Journal of Fluency Disorders},
  year     = {2001},
  volume   = {26},
  number   = {1},
  pages    = {1 - 15},
  doi      = {http://dx.doi.org/10.1016/S0094-730X(00)00090-5},
  file     = {:Speech Therapy/Vanryckeghem2001.pdf:PDF},
  groups   = {Speech, ACII Doctoral Consortium},
  issn     = {0094-730X},
  keywords = {Communication Attitude Test, Attitude, Emotion, Stuttering, quantitative Evaluation, children},
  url      = {http://www.sciencedirect.com/science/article/pii/S0094730X00000905},
}

@InProceedings{Zhou2016,
  author        = {Zhou, Jing and Hong, Xiaopeng and Su, Fei and Zhao, Guoying},
  title         = {Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video},
  booktitle     = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  year          = {2016},
  pages         = {84--92},
  __markedentry = {[CarlaViegas:5]},
  file          = {:FacialExpressions/Zhou2016.pdf:PDF},
  groups        = {Facial Expressions, To Read},
  keywords      = {facial expressions, pain},
}

@InProceedings{Almutiry2016,
  author       = {Almutiry, Riyadh and Couth, Samuel and Poliakoff, Ellen and Kotz, Sonja and Silverdale, Monty and Cootes, Tim},
  title        = {Facial Behaviour Analysis in Parkinson’s Disease},
  booktitle    = {International Conference on Medical Imaging and Virtual Reality},
  year         = {2016},
  pages        = {329--339},
  organization = {Springer},
  groups       = {Facial Expressions},
  keywords     = {rank4, Parkinson, facial expressions},
}

@InProceedings{Anil2016,
  author    = {J. Anil and L. P. Suresh},
  title     = {Literature survey on face and face expression recognition},
  booktitle = {2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)},
  year      = {2016},
  pages     = {1-6},
  month     = {March},
  doi       = {10.1109/ICCPCT.2016.7530173},
  file      = {:FacialExpressions/Anil2016.pdf:PDF},
  groups    = {Survey, , and Included in Excel Sheet, Face Recognition, Read and Included in Excel Sheet},
  keywords  = {differential geometry;emotion recognition;face recognition;feature extraction;image matching;image registration;image texture;FER algorithms;bag of words method;computer vision;curvelet feature extraction;face expression recognition;facial image processing;gradient feature matching;local directional number pattern;patched geodesic texture transform;regional registration technique;Algorithm design and analysis;Databases;Face;Face recognition;Feature extraction;Three-dimensional displays;Transforms;Bag of Words Method;Curvelet Feature Extraction;Face Expression Recognition;Feature Extraction;Gradient Feature Matching;Local Directional Number Pattern;Patched Geodesic Texture Transform;Regional Registration Technique, rank1, survey, facial expressions},
}

@Article{Bandini2017,
  author    = {Andrea Bandini and Silvia Orlandi and Hugo Jair Escalante and Fabio Giovannelli and Massimo Cincotta and Carlos A. Reyes-Garcia and Paola Vanni and Gaetano Zaccara and Claudia Manfredi},
  title     = {Analysis of facial expressions in parkinson's disease through video-based automatic methods},
  journal   = {Journal of Neuroscience Methods},
  year      = {2017},
  volume    = {281},
  pages     = {7 - 20},
  abstract  = {AbstractBackground The automatic analysis of facial expressions is an evolving field that finds several clinical applications. One of these applications is the study of facial bradykinesia in Parkinson’s disease (PD), which is a major motor sign of this neurodegenerative illness. Facial bradykinesia consists in the reduction/loss of facial movements and emotional facial expressions called hypomimia. New method In this work we propose an automatic method for studying facial expressions in \{PD\} patients relying on video-based Methods 17 Parkinsonian patients and 17 healthy control subjects were asked to show basic facial expressions, upon request of the clinician and after the imitation of a visual cue on a screen. Through an existing face tracker, the Euclidean distance of the facial model from a neutral baseline was computed in order to quantify the changes in facial expressivity during the tasks. Moreover, an automatic facial expressions recognition algorithm was trained in order to study how \{PD\} expressions differed from the standard expressions. Results Results show that control subjects reported on average higher distances than \{PD\} patients along the tasks. Comparison with existing methods This confirms that control subjects show larger movements during both posed and imitated facial expressions. Moreover, our results demonstrate that anger and disgust are the two most impaired expressions in \{PD\} patients. Conclusions Contactless video-based systems can be important techniques for analyzing facial expressions also in rehabilitation, in particular speech therapy, where patients could get a definite advantage from a real-time feedback about the proper facial expressions/movements to perform. },
  doi       = {https://doi.org/10.1016/j.jneumeth.2017.02.006},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Bandini2017.pdf:PDF},
  issn      = {0165-0270},
  keywords  = {Parkinson's disease, Hypomimia, Automatic facial expression recognition, Facial mimicry, Contactless, Video-based, facial Expressions, Parkinson, relevant},
  url       = {http://www.sciencedirect.com/science/article/pii/S0165027017300481},
}

@Article{Cadieux1997,
  author    = {CADIEUX, NICOLE L. and GREVE, KEVIN W.},
  title     = {Emotion processing in Alzheimer's disease},
  journal   = {Journal of the International Neuropsychological Society},
  year      = {1997},
  volume    = {3},
  number    = {5},
  pages     = {411–419},
  abstract  = {Emotion processing deficits may have an important effect on the quality of life of Alzheimer's disease (AD) patients and their families, yet there are few studies in this area and little is known about the cause of such deficits in AD. This study sought to determine if some AD patients have a disruption in a specific right hemisphere emotion processing system, and to determine if the processing of emotional facial expression is more vulnerable to the pathology of AD than is the perception of emotional prosody. It was specifically hypothesized that patients with greater right hemisphere dysfunction (low spatial AD patients) would be impaired on emotion processing tasks relative to those with predominantly left hemisphere dysfunction (low verbal AD patients). Both groups showed impairment on emotion processing tasks but for different reasons. The low verbal patients performed poorly on the affect processing measures because they had difficulty comprehending and/or remembering the task instructions. In contrast, low spatial AD patients have emotion processing deficits that are independent of language and/or memory and may be due to a more general visuoperceptual deficit that affects the perception of static but not dynamic affective stimuli. (JINS, 1997, 3, 411–419.)},
  keywords  = {Alzheimer, facial expressions, emotional speech, differences in AD patients},
  publisher = {Cambridge University Press},
}

@Article{Chao2016,
  author    = {Linlin Chao and Jianhua Tao and Minghao Yang and Ya Li and Zhengqi Wen},
  title     = {Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention},
  journal   = {CoRR},
  year      = {2016},
  volume    = {abs/1603.08321},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChaoTYLW16},
  file      = {:FacialExpressions/Chao2016.pdf:PDF},
  groups    = {To Read, Facial Expressions},
  keywords  = {facial expressions, audio-visual emotion},
  timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
  url       = {http://arxiv.org/abs/1603.08321},
}

@Article{Chu2016,
  author    = {Wen{-}Sheng Chu and Fernando De la Torre and Jeffrey F. Cohn},
  title     = {Modeling Spatial and Temporal Cues for Multi-label Facial Action Unit Detection},
  journal   = {CoRR},
  year      = {2016},
  volume    = {abs/1608.00911},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChuTC16},
  file      = {:FacialExpressions/Chu2016.pdf:PDF},
  groups    = {Facial Expressions},
  keywords  = {facial expressions},
  timestamp = {Fri, 02 Sep 2016 17:46:24 +0200},
  url       = {http://arxiv.org/abs/1608.00911},
}

@InBook{Cohn2014,
  pages     = {131},
  title     = {Automated face analysis for affective},
  publisher = {Oxford University Press, USA},
  year      = {2014},
  author    = {Cohn, Jeffrey F and De la Torre, Fernando},
  file      = {:FacialExpressions/Cohn2015.pdf:PDF},
  groups    = {Survey, Facial Expressions, Read and Included in Excel Sheet},
  journal   = {The Oxford handbook of affective computing},
  keywords  = {survey, facial expressions},
}

@InProceedings{Cotter2010,
  author       = {Cotter, Shane F},
  title        = {Sparse representation for accurate classification of corrupted and occluded facial expressions},
  booktitle    = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on},
  year         = {2010},
  pages        = {838--841},
  organization = {IEEE},
  groups       = {Facial Expressions},
  keywords     = {facial expressions, sparse representation},
}

@Article{Danelakis2015,
  author   = {Danelakis, Antonios and Theoharis, Theoharis and Pratikakis, Ioannis},
  title    = {A survey on facial expression recognition in 3D video sequences},
  journal  = {Multimedia Tools and Applications},
  year     = {2015},
  volume   = {74},
  number   = {15},
  pages    = {5577--5615},
  abstract = {Facial expression recognition constitutes an active research area due to its various applications. This survey addresses methodologies for 3D mesh video facial expression recognition. Recognition is, actually, a special case of intra-class retrieval. The approaches are analyzed and compared in detail. They are primarily categorized according to the 3D dynamic face analysis technique used. In addition, currently available datasets, used for 3D video facial expression analysis, are presented. Finally, future challenges that can be addressed in order for 3D video facial expression recognition field to be further improved, are extensively discussed.},
  doi      = {10.1007/s11042-014-1869-6},
  file     = {:FacialExpressions/Danelakis2015.pdf:PDF},
  groups   = {Survey},
  issn     = {1573-7721},
  keywords = {survey, facial Expressions, 3d},
  url      = {http://dx.doi.org/10.1007/s11042-014-1869-6},
}

@InProceedings{de2015intraface,
  author       = {De la Torre, Fernando and Chu, Wen-Sheng and Xiong, Xuehan and Vicente, Francisco and Ding, Xiaoyu and Cohn, Jeffrey},
  title        = {Intraface},
  booktitle    = {Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on},
  year         = {2015},
  volume       = {1},
  pages        = {1--8},
  organization = {IEEE},
  groups       = {Facial Expressions},
  keywords     = {facial expressions},
}

@Article{Ekman1977,
  author    = {Ekman, Paul and Friesen, Wallace V},
  title     = {Facial action coding system},
  year      = {1977},
  groups    = {Facial Expressions},
  keywords  = {facial expressions, fundamentals},
  publisher = {Consulting Psychologists Press, Stanford University, Palo Alto},
}

@Book{Ekman1997,
  title     = {What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)},
  publisher = {Oxford University Press, USA},
  year      = {1997},
  author    = {Ekman, Paul and Rosenberg, Erika L},
  groups    = {Facial Expressions},
  keywords  = {facial expressions, fundamentals},
}

@Article{Fasel2003,
  author   = {B. Fasel and Juergen Luettin},
  title    = {Automatic facial expression analysis: a survey},
  journal  = {Pattern Recognition},
  year     = {2003},
  volume   = {36},
  number   = {1},
  pages    = {259 - 275},
  doi      = {http://dx.doi.org/10.1016/S0031-3203(02)00052-3},
  file     = {:FacialExpressions/Fasel2003.pdf:PDF},
  groups   = {Survey, Facial Expressions},
  issn     = {0031-3203},
  keywords = {Facial expression recognition, Facial expression interpretation, Emotion recognition, Affect recognition, FACS, survey, facial expressions},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320302000523},
}

@Article{Gola2017,
  author   = {Kelly A. Gola and Tal Shany-Ur and Peter Pressman and Isa Sulman and Eduardo Galeana and Hillary Paulsen and Lauren Nguyen and Teresa Wu and Babu Adhimoolam and Pardis Poorzand and Bruce L. Miller and Katherine P. Rankin},
  title    = {A neural network underlying intentional emotional facial expression in neurodegenerative disease},
  journal  = {NeuroImage: Clinical},
  year     = {2017},
  volume   = {14},
  pages    = {672 - 678},
  abstract = {Abstract Intentional facial expression of emotion is critical to healthy social interactions. Patients with neurodegenerative disease, particularly those with right temporal or prefrontal atrophy, show dramatic socioemotional impairment. This was an exploratory study examining the neural and behavioral correlates of intentional facial expression of emotion in neurodegenerative disease patients and healthy controls. One hundred and thirty three participants (45 Alzheimer's disease, 16 behavioral variant frontotemporal dementia, 8 non-fluent primary progressive aphasia, 10 progressive supranuclear palsy, 11 right-temporal frontotemporal dementia, 9 semantic variant primary progressive aphasia patients and 34 healthy controls) were video recorded while imitating static images of emotional faces and producing emotional expressions based on verbal command; the accuracy of their expression was rated by blinded raters. Participants also underwent face-to-face socioemotional testing and informants described participants' typical socioemotional behavior. Patients' performance on emotion expression tasks was correlated with gray matter volume using voxel-based morphometry (VBM) across the entire sample. We found that intentional emotional imitation scores were related to fundamental socioemotional deficits; patients with known socioemotional deficits performed worse than controls on intentional emotion imitation; and intentional emotional expression predicted caregiver ratings of empathy and interpersonal warmth. Whole brain \{VBMs\} revealed a rightward cortical atrophy pattern homologous to the left lateralized speech production network was associated with intentional emotional imitation deficits. Results point to a possible neural mechanisms underlying complex socioemotional communication deficits in neurodegenerative disease patients. },
  doi      = {https://doi.org/10.1016/j.nicl.2017.01.016},
  file     = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/FacialExpressions/Gola2017.pdf:PDF},
  issn     = {2213-1582},
  keywords = {Frontotemporal dementia, Emotion, Social functioning, Empathy, Facial expressions , Alzheimer},
  url      = {http://www.sciencedirect.com/science/article/pii/S2213158217300165},
}

@Article{Zaytseva2014,
  author    = {Han, Kyung-Hun and Zaytseva, Yuliya and Bao, Yan and P{\"o}ppel, Ernst and Chung, Sun Yong and Kim, Jong Woo and Kim, Hyun Taek},
  title     = {Impairment of vocal expression of negative emotions in patients with Alzheimer’s disease},
  journal   = {Frontiers in aging neuroscience},
  year      = {2014},
  volume    = {6},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Zaytseva2014.pdf:PDF},
  keywords  = {Alzheimer, facial expressions, emotional speech,},
  publisher = {Frontiers Media SA},
}

@InProceedings{Holt2016,
  author    = {R. A. Holt and S. Jones and G. Washington},
  title     = {Measuring Stress, Anxiety, and Depression in PTSD Sufferers Using Micro-Movements from Video},
  booktitle = {2016 IEEE International Conference on Healthcare Informatics (ICHI)},
  year      = {2016},
  pages     = {483-487},
  month     = {Oct},
  doi       = {10.1109/ICHI.2016.88},
  file      = {:FacialExpressions/Holt2016.pdf:PDF},
  groups    = {Facial Expressions, To Read},
  keywords  = {cardiology;medical disorders;medical image processing;neurophysiology;skin;video signal processing;EVM method;Euler video magnification;HRV;PTSD sufferers;YouTube videos;anxiety analysis;depression analysis;fitness monitor;heart rate variability;post-traumatic stress disorder;startle reactions;stress measurement;video micromovements;Heart rate variability;Image color analysis;Laplace equations;Skin;Stress;YouTube;BPM;HRV;PTSD;anxiety;depression;health;micro-movements;stress, facial expressions},
}

@InProceedings{Huang2010,
  author       = {Huang, Kuan-Chieh and Huang, Sheng-Yu and Kuo, Yau-Hwang},
  title        = {Emotion recognition based on a novel triangular facial feature extraction method},
  booktitle    = {Neural Networks (IJCNN), The 2010 International Joint Conference on},
  year         = {2010},
  pages        = {1--6},
  organization = {IEEE},
  file         = {:FacialExpressions/Huang2010.pdf:PDF},
  groups       = {Facial Expressions},
  keywords     = {facial expressions, feature extraction},
}

@InProceedings{Jaiswal2016,
  author       = {Jaiswal, Shashank and Valstar, Michel},
  title        = {Deep learning the dynamic appearance and shape of facial action units},
  booktitle    = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year         = {2016},
  pages        = {1--8},
  organization = {IEEE},
  file         = {:FacialExpressions/Jaiswal2016.pdf:PDF},
  groups       = {Facial Expressions},
  keywords     = {facial expressions, deep learning},
}

@Article{Livingstone2016,
  author    = {Livingstone, Steven R and Vezer, Esztella and McGarry, Lucy M and Lang, Anthony E and Russo, Frank A},
  title     = {Deficits in the Mimicry of Facial Expressions in Parkinson's Disease},
  journal   = {Frontiers in Psychology},
  year      = {2016},
  volume    = {7},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Livingstone2016.pdf:PDF},
  keywords  = {facial Expressions, parkinson, behaviomedics},
  publisher = {Frontiers Media SA},
}

@Book{Lucey2007,
  title     = {Investigating spontaneous facial action recognition through aam representations of the face},
  publisher = {INTECH Open Access Publisher},
  year      = {2007},
  author    = {Lucey, Simon and Ashraf, Ahmed Bilal and Cohn, Jeffrey F},
  file      = {:Books/SpontaneousFacialActionRecognition.pdf:PDF},
  groups    = {Facial Expressions},
  keywords  = {facial expressions},
}

@InCollection{Martinez2016,
  author    = {Martinez, Brais and Valstar, Michel F},
  title     = {Advances, challenges, and opportunities in automatic facial expression recognition},
  booktitle = {Advances in Face Detection and Facial Image Analysis},
  publisher = {Springer},
  year      = {2016},
  pages     = {63--100},
  file      = {:FacialExpressions/Martinez2016.pdf:PDF},
  groups    = {Facial Expressions, Survey},
  keywords  = {survey, facial expressions},
}

@InProceedings{Mishra2015,
  author    = {B. Mishra and S. L. Fernandes and K. Abhishek and A. Alva and C. Shetty and C. V. Ajila and D. Shetty and H. Rao and P. Shetty},
  title     = {Facial expression recognition using feature based techniques and model based techniques: A survey},
  booktitle = {2015 2nd International Conference on Electronics and Communication Systems (ICECS)},
  year      = {2015},
  pages     = {589-594},
  month     = {Feb},
  doi       = {10.1109/ECS.2015.7124976},
  file      = {:FacialExpressions/Mishra2015.pdf:PDF},
  groups    = {Survey},
  keywords  = {facial expressions ;curvelet transforms;emotion recognition;face recognition;feature extraction;image classification;image texture;video signal processing;AR public face database;BU-3DFE public face database;Bosphorous public face database;CK+ public face database;CMU-PIE public face database;FERET public face database;FRGC public face database;FRGCv2 database;GEMEP-FERA public face database;Georgia tech public face database;JAFFE public face database;LFW public face database;MMI public face database;authentication;automatic feedback capture;computer systems;eNTERFACE 05 public face database;facial expression recognition technique;feature based curvelet approach;feature based techniques;gadgets;identification;model based techniques;model based textured 3D video technique;nonverbal communication;verification;Databases;Emotion recognition;Face;Face recognition;Feature extraction;Shape;Training;curvelet based feature extraction;expression invariant recognition;facial expression recognition;feature based techniques;model based techniques;textured 3D video, survey},
}

@InProceedings{Nicolle2012,
  author       = {Nicolle, J{\'e}r{\'e}mie and Rapp, Vincent and Bailly, K{\'e}vin and Prevost, Lionel and Chetouani, Mohamed},
  title        = {Robust continuous prediction of human emotions using multiscale dynamic cues},
  booktitle    = {Proceedings of the 14th ACM international conference on Multimodal interaction},
  year         = {2012},
  pages        = {501--508},
  organization = {ACM},
  file         = {:FacialExpressions/Nicolle2012.pdf:PDF},
  groups       = {Facial Expressions},
  keywords     = {facial expressions, audio-visual emotion},
}

@Article{Pantic2000,
  author    = {Pantic, Maja and Rothkrantz, Leon J. M.},
  title     = {Automatic analysis of facial expressions: The state of the art},
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  year      = {2000},
  volume    = {22},
  number    = {12},
  pages     = {1424--1445},
  groups    = {Facial Expressions},
  keywords  = {facial expressions},
  publisher = {IEEE},
}

@Article{Ricciardi2015,
  author    = {Ricciardi, Lucia and Bologna, Matteo and Morgante, Francesca and Ricciardi, Diego and Morabito, Bruno and Volpe, Daniele and Martino, Davide and Tessitore, Alessandro and Pomponi, Massimiliano and Bentivoglio, Anna Rita and others},
  title     = {Reduced facial expressiveness in Parkinson's disease: A pure motor disorder?},
  journal   = {Journal of the Neurological Sciences},
  year      = {2015},
  volume    = {358},
  number    = {1},
  pages     = {125--130},
  file      = {:Speech Therapy/Ricciardi2015.pdf:PDF},
  groups    = {Speech},
  keywords  = {Parkinson, facial Expressions},
  publisher = {Elsevier},
}

@InProceedings{Ringeval2015,
  author    = {Ringeval, Fabien and Schuller, Bj\"{o}rn and Valstar, Michel and Jaiswal, Shashank and Marchi, Erik and Lalanne, Denis and Cowie, Roddy and Pantic, Maja},
  title     = {AV+EC 2015: The First Affect Recognition Challenge Bridging Across Audio, Video, and Physiological Data},
  booktitle = {Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge},
  year      = {2015},
  series    = {AVEC '15},
  pages     = {3--8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2811642},
  doi       = {10.1145/2808196.2811642},
  file      = {:FacialExpressions/Ringeval2015.pdf:PDF},
  groups    = {Facial Expressions, Multimodal, To Read},
  isbn      = {978-1-4503-3743-4},
  keywords  = {affective computing, challenge, emotion recognition, facial expression, physiological signal, speech, multimodal, facial expressions, audio-visual Emotion, physiological signals Emotion},
  location  = {Brisbane, Australia},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/2808196.2811642},
}

@Article{Roychowdhury2015survey,
  author   = {Roychowdhury, Sohini and Emmons, Michelle},
  title    = {A survey of the trends in facial and expression recognition databases and methods},
  journal  = {arXiv preprint arXiv:1511.02407},
  year     = {2015},
  file     = {:FacialExpressions/Roychowdhury2015.pdf:PDF},
  groups   = {Facial Expressions},
  keywords = {facial expressions, survey, database},
}

@Article{Sandbach2012survey,
  author    = {Sandbach, Georgia and Zafeiriou, Stefanos and Pantic, Maja and Yin, Lijun},
  title     = {Static and dynamic 3D facial expression recognition: A comprehensive survey},
  journal   = {Image and Vision Computing},
  year      = {2012},
  volume    = {30},
  number    = {10},
  pages     = {683--697},
  file      = {:FacialExpressions/Sandbach2012survey.pdf:PDF},
  keywords  = {3d, facial Expressions, survey},
  publisher = {Elsevier},
}

@Article{Sariyanidi2015,
  author    = {Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea},
  title     = {Automatic analysis of facial affect: A survey of registration, representation, and recognition},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2015},
  volume    = {37},
  number    = {6},
  pages     = {1113--1133},
  file      = {:FacialExpressions/Sariyanidi2015.pdf:PDF},
  groups    = {Facial Expressions},
  keywords  = {facial expressions},
  publisher = {IEEE},
}

@InProceedings{Sariyanidi2013,
  author    = {Sariyanidi, Evangelos and Gunes, Hatice and G{\"o}kmen, Muhittin and Cavallaro, Andrea},
  title     = {Local Zernike Moment Representation for Facial Affect Recognition.},
  booktitle = {BMVC},
  year      = {2013},
  keywords  = {facial expressions},
}

@Article{Savran2012,
  author   = {Arman Savran and Bülent Sankur and M. Taha Bilge},
  title    = {Comparative evaluation of 3D vs. 2D modality for automatic detection of facial action units},
  journal  = {Pattern Recognition},
  year     = {2012},
  volume   = {45},
  number   = {2},
  pages    = {767 - 782},
  abstract = {Automatic detection of facial expressions attracts great attention due to its potential applications in human–computer interaction as well as in human facial behavior research. Most of the research has so far been performed in 2D. However, as the limitations of 2D data are understood, expression analysis research is being pursued in 3D face modality. 3D can capture true facial surface data and is less disturbed by illumination and head pose. At this junction we have conducted a comparative evaluation of 3D and 2D face modalities. We have investigated extensively 25 action units (AU) defined in the Facial Action Coding System. For fairness we map facial surface geometry into 2D and apply totally data-driven techniques in order to avoid biases due to design. We have demonstrated that overall 3D data performs better, especially for lower face \{AUs\} and that there is room for improvement by fusion of 2D and 3D modalities. Our study involves the determination of the best feature set from 2D and 3D modalities and of the most effective classifier, both from several alternatives. Our detailed analysis puts into evidence the merits and some shortcomings of 3D modality over 2D in classifying facial expressions from single images. },
  doi      = {http://dx.doi.org/10.1016/j.patcog.2011.07.022},
  file     = {:FacialExpressions/Savran2011.pdf:PDF},
  groups   = {Facial Expressions},
  issn     = {0031-3203},
  keywords = {3D expression recognition, 3D facial expression database, Action unit detection, Facial action coding system, Modality fusion, Gabor wavelets, facial expressions, 3d},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320311003104},
}

@Article{Sidequersky2016,
  author    = {Sidequersky, Fernanda V and Mapelli, Andrea and Annoni, Isabella and Zago, Matteo and De Fel{\'\i}cio, Cl{\'a}udia M and Sforza, Chiarella},
  title     = {Three-dimensional motion analysis of facial movement during verbal and nonverbal expressions in healthy subjects},
  journal   = {Clinical Anatomy},
  year      = {2016},
  volume    = {29},
  number    = {8},
  pages     = {991--997},
  file      = {:FacialExpressions/Sidequersky2016.pdf:PDF},
  groups    = {Facial Expressions},
  keywords  = {facial expressions, 3d, non-verbal communication},
  publisher = {Wiley Online Library},
}

@InProceedings{Silverdale2016,
  author       = {Silverdale, Monty and Cootes, Tim},
  title        = {Facial Behaviour Analysis in Parkinson’s Disease},
  booktitle    = {Medical Imaging and Augmented Reality: 7th International Conference, MIAR 2016, Bern, Switzerland, August 24-26, 2016, Proceedings},
  year         = {2016},
  volume       = {9805},
  pages        = {329},
  organization = {Springer},
  keywords     = {Parkinson, facial Expressions,},
  url          = {https://books.google.pt/books?hl=pt-PT&lr=&id=CAbWDAAAQBAJ&oi=fnd&pg=PA329&dq=emotion+expression+speech+parkinson%27s&ots=JNWMeZCVL8&sig=rEQ_nbQQnjT4FGJWZY-BrAyUQ6U&redir_esc=y#v=onepage&q=emotion%20expression%20speech%20parkinson's&f=false},
}

@InProceedings{Simon2010,
  author       = {Simon, Tomas and Nguyen, Minh Hoai and De La Torre, Fernando and Cohn, Jeffrey F},
  title        = {Action unit detection with segment-based svms},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  year         = {2010},
  pages        = {2737--2744},
  organization = {IEEE},
  file         = {:FacialExpressions/Simon2010.pdf:PDF},
  groups       = {Facial Expressions},
  keywords     = {facial expressions},
}

@Article{Tian2001,
  author    = {Tian, Y-I and Kanade, Takeo and Cohn, Jeffrey F},
  title     = {Recognizing action units for facial expression analysis},
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  year      = {2001},
  volume    = {23},
  number    = {2},
  pages     = {97--115},
  groups    = {Facial Expressions},
  keywords  = {facial expressions},
  publisher = {IEEE},
}

@InCollection{Tian2005,
  author    = {Tian, Ying-Li and Kanade, Takeo and Cohn, Jeffrey F},
  title     = {Facial expression analysis},
  booktitle = {Handbook of face recognition},
  publisher = {Springer},
  year      = {2005},
  pages     = {247--275},
  groups    = {Facial Expressions, Face Recognition},
  keywords  = {facial expressions, face recognition},
}

@InProceedings{Valstar2014automatic,
  author       = {Valstar, Michel},
  title        = {Automatic behaviour understanding in medicine},
  booktitle    = {Proceedings of the 2014 Workshop on Roadmapping the Future of Multimodal Interaction Research including Business Opportunities and Challenges},
  year         = {2014},
  pages        = {57--60},
  organization = {ACM},
  file         = {:FacialExpressions/Valstar2014.pdf:PDF},
  groups       = {Facial Expressions},
  keywords     = {facial expressions, behaviomedics, healthcare, anxiety, autism, schizophrenia, pain},
}

@InProceedings{Valstar2016avec,
  author       = {Valstar, Michel and Gratch, Jonathan and Schuller, Bj{\"o}rn and Ringeval, Fabien and Lalanne, Dennis and Torres Torres, Mercedes and Scherer, Stefan and Stratou, Giota and Cowie, Roddy and Pantic, Maja},
  title        = {AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge},
  booktitle    = {Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge},
  year         = {2016},
  pages        = {3--10},
  organization = {ACM},
  file         = {:FacialExpressions/AVEC2016.pdf:PDF},
  groups       = {Facial Expressions},
  keywords     = {facial expressions, depression, behaviomedics},
}

@InProceedings{Valstar2006,
  author    = {Valstar, Michel and Pantic, Maja},
  title     = {Fully Automatic Facial Action Unit Detection and Temporal Analysis},
  booktitle = {Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition Workshop},
  year      = {2006},
  series    = {CVPRW '06},
  pages     = {149--},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1153856},
  doi       = {10.1109/CVPRW.2006.85},
  file      = {:FacialExpressions/Valstar2006.pdf:PDF},
  groups    = {Facial Expressions},
  isbn      = {0-7695-2646-2},
  keywords  = {facial expressions},
  url       = {http://dx.doi.org/10.1109/CVPRW.2006.85},
}

@Article{Valstar2017,
  author        = {{Valstar}, M.~F. and {S{\'a}nchez-Lozano}, E. and {Cohn}, J.~F. and {Jeni}, L.~A. and {Girard}, J.~M. and {Zhang}, Z. and {Yin}, L. and {Pantic}, M.},
  title         = {{FERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = feb,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170204174V},
  archiveprefix = {arXiv},
  eprint        = {1702.04174},
  file          = {:FacialExpressions/Valstar2017.pdf:PDF},
  groups        = {Facial Expressions},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, facial expressions},
  primaryclass  = {cs.CV},
}

@Article{Zeng2009survey,
  author    = {Zeng, Zhihong and Pantic, Maja and Roisman, Glenn I and Huang, Thomas S},
  title     = {A survey of affect recognition methods: Audio, visual, and spontaneous expressions},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2009},
  volume    = {31},
  number    = {1},
  pages     = {39--58},
  file      = {:FacialExpressions/Zeng2009survey.pdf:PDF},
  groups    = {Facial Expressions},
  keywords  = {facial expressions, survey, audio-visual Emotion},
  publisher = {IEEE},
}

@Article{Leppanen2017,
  author    = {Leppanen, Jenni and Dapelo, Marcela Marin and Davies, Helen and Lang, Katie and Treasure, Janet and Tchanturia, Kate},
  title     = {Computerised analysis of facial emotion expression in eating disorders},
  journal   = {PloS one},
  year      = {2017},
  volume    = {12},
  number    = {6},
  pages     = {e0178972},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Leppanen2017.pdf:PDF},
  keywords  = {facial Expression, eating disorder, facial expressions},
  publisher = {Public Library of Science},
}

@InProceedings{Schuller2015interspeech,
  author    = {Schuller, Bj{\"o}rn and Steidl, Stefan and Batliner, Anton and Hantke, Simone and H{\"o}nig, Florian and Orozco-Arroyave, Juan Rafael and N{\"o}th, Elmar and Zhang, Yue and Weninger, Felix},
  title     = {The INTERSPEECH 2015 Computational Paralinguistics Challenge: Nativeness, Parkinson's \& Eating Condition},
  booktitle = {Sixteenth Annual Conference of the International Speech Communication Association},
  year      = {2015},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Schuller2015interspeech.pdf:PDF},
  keywords  = {emotion speech, parkinson, behaviomedics, database},
}

@Article{Anagnostopoulos2015survey,
  author    = {Anagnostopoulos, Christos-Nikolaos and Iliou, Theodoros and Giannoukos, Ioannis},
  title     = {Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011},
  journal   = {Artificial Intelligence Review},
  year      = {2015},
  volume    = {43},
  number    = {2},
  pages     = {155--177},
  file      = {:Speech+Emotion/survey.pdf:PDF},
  keywords  = {survey, emotional speech},
  publisher = {Springer},
}

@PhdThesis{Cummins2016,
  author   = {Cummins, N},
  title    = {Automatic Assessment of Depression from Speech: Paralinguistic Analysis, Modelling and Machine Learning},
  school   = {PhD Thesis},
  year     = {2016},
  file     = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Speech Therapy/Cummins2016.pdf:PDF},
  keywords = {depression, emotional speech, machine learning, paralinguistic, automatic assessment},
}

@Article{Lopez-de-Ipina2015,
  author   = {K. López-de-Ipiña and J.B. Alonso-Hernández and J. Solé-Casals and C.M. Travieso-González and A. Ezeiza and M. Faúndez-Zanuy and P.M. Calvo and B. Beitia},
  title    = {Feature selection for automatic analysis of emotional response based on nonlinear speech modeling suitable for diagnosis of Alzheimer׳s disease},
  journal  = {Neurocomputing},
  year     = {2015},
  volume   = {150, Part B},
  pages    = {392 - 401},
  note     = {Special Issue on Information Processing and Machine Learning for Applications of EngineeringSolving Complex Machine Learning Problems with Ensemble MethodsVisual Analytics using Multidimensional ProjectionsSelected papers from the \{IEEE\} 17th International Conference on Intelligent Engineering Systems (INES’13)Selected papers from the Workshop on Visual Analytics using Multidimensional Projections, held at EuroVis 2013},
  abstract = {Abstract Alzheimer׳s disease (AD) is the most common type of dementia among the elderly. This work is part of a larger study that aims to identify novel technologies and biomarkers or features for the early detection of \{AD\} and its degree of severity. The diagnosis is made by analyzing several biomarkers and conducting a variety of tests (although only a post-mortem examination of the patients’ brain tissue is considered to provide definitive confirmation). Non-invasive intelligent diagnosis techniques would be a very valuable diagnostic aid. This paper concerns the Automatic Analysis of Emotional Response (AAER) in spontaneous speech based on classical and new emotional speech features: Emotional Temperature (ET) and fractal dimension (FD). This is a pre-clinical study aiming to validate tests and biomarkers for future diagnostic use. The method has the great advantage of being non-invasive, low cost, and without any side effects. The \{AAER\} shows very promising results for the definition of features useful in the early diagnosis of AD. },
  doi      = {https://doi.org/10.1016/j.neucom.2014.05.083},
  file     = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Lopez-de-Ipiña2015.pdf:PDF},
  issn     = {0925-2312},
  keywords = {Emotional response, Automatic speech analysis, Emotion recognition, Non-linear modeling, Fractal dimension, Emotional temperature , behaviomedics, alzheimer, emotional Speech},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231214012958},
}

@Article{DouglasCowie2003,
  author   = {Ellen Douglas-Cowie and Nick Campbell and Roddy Cowie and Peter Roach},
  title    = {Emotional speech: Towards a new generation of databases},
  journal  = {Speech Communication},
  year     = {2003},
  volume   = {40},
  number   = {1–2},
  pages    = {33 - 60},
  abstract = {Research on speech and emotion is moving from a period of exploratory research into one where there is a prospect of substantial applications, notably in human–computer interaction. Progress in the area relies heavily on the development of appropriate databases. This paper addresses four main issues that need to be considered in developing databases of emotional speech: scope, naturalness, context and descriptors. The state of the art is reviewed. A good deal has been done to address the key issues, but there is still a long way to go. The paper shows how the challenge of developing appropriate databases is being addressed in three major recent projects––the Reading–Leeds project, the Belfast project and the CREST–ESP project. From these and other studies the paper draws together the tools and methods that have been developed, addresses the problems that arise and indicates the future directions for the development of emotional speech databases. },
  doi      = {http://dx.doi.org/10.1016/S0167-6393(02)00070-5},
  file     = {:Audiovisual Speech Recognition/Cowie2003.pdf:PDF},
  groups   = {Audiovisual speech},
  issn     = {0167-6393},
  keywords = {Databases, Emotional speech, Scope, Naturalness, Context, Descriptors},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639302000705},
}

@Article{Horley2010,
  author   = {Horley, Kaye and Reid, Amanda and Burnham, Denis},
  title    = {Emotional Prosody Perception and Production in Dementia of the Alzheimer’s Type},
  journal  = {Journal of Speech, Language, and Hearing Research},
  year     = {2010},
  volume   = {53},
  number   = {5},
  pages    = {1132-1146},
  abstract = {Purpose In this study, the authors investigated emotional prosody in patients with moderate Dementia of the Alzheimer’s type (DAT) With Late Onset. It was expected that both expression and reception of prosody would be impaired relative to age-matched controls.

MethodT wenty DAT and 20 control participants engaged in 2 expressive and 2 receptive tasks with randomly presented exemplars of sentences targeting the emotions of happiness, anger, sadness, and surprise.

Results In the expressive tasks, objective acoustic measurements revealed significantly less pitch modulation by the patient group, but these measurements showed that they retained the ability to vary pitch level, pitch modulation, and speaking rate as a function of emotion. In the receptive tasks, perception of emotion by the patient group was significantly inferior to the control group.

Conclusions Implications are discussed regarding impaired emotional prosody in DAT, and the utility of objective acoustic measures in revealing subtle deficits and overcoming methodological inconsistencies is emphasized. Further research is critical in advancing our understanding of this pervasive disorder and is important, clinically, in the provision of specific interventions.},
  doi      = {10.1044/1092-4388(2010/09-0030)},
  keywords = {voice prosody, emotional speech, alzheimer},
  url      = { + http://dx.doi.org/10.1044/1092-4388(2010/09-0030)},
}

@Article{Lopez-de-Ipina2015b,
  author   = {L{\'o}pez-de-Ipi{\~{n}}a, K. and Alonso, J. B. and Sol{\'e}-Casals, J. and Barroso, N. and Henriquez, P. and Faundez-Zanuy, M. and Travieso, C. M. and Ecay-Torres, M. and Mart{\'i}nez-Lage, P. and Eguiraun, H.},
  title    = {On Automatic Diagnosis of Alzheimer's Disease Based on Spontaneous Speech Analysis and Emotional Temperature},
  journal  = {Cognitive Computation},
  year     = {2015},
  volume   = {7},
  number   = {1},
  pages    = {44--55},
  abstract = {Alzheimer's disease (AD) is the most prevalent form of progressive degenerative dementia; it has a high socioeconomic impact in Western countries. Therefore, it is one of the most active research areas today. Alzheimer's disease is sometimes diagnosed by excluding other dementias, and definitive confirmation is only obtained through a postmortem study of the brain tissue of the patient. The work presented here is part of a larger study that aims to identify novel technologies and biomarkers for early AD detection, and it focuses on evaluating the suitability of a new approach for early diagnosis of AD by noninvasive methods. The purpose is to examine, in a pilot study, the potential of applying machine learning algorithms to speech features obtained from suspected Alzheimer's disease sufferers in order to help diagnose this disease and determine its degree of severity. Two human capabilities relevant in communication have been analyzed for feature selection: spontaneous speech and emotional response. The experimental results obtained were very satisfactory and promising for the early diagnosis and classification of AD patients.},
  doi      = {10.1007/s12559-013-9229-9},
  file     = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Lopez-de-Ipiña2015b.pdf:PDF},
  issn     = {1866-9964},
  keywords = {emotional speech, alzheimer, behaviomedics, diagnosis, emotional temperature},
  url      = {http://dx.doi.org/10.1007/s12559-013-9229-9},
}

@InBook{Lopez-de-Ipina2014,
  pages     = {272--281},
  title     = {On the Alzheimer's Disease Diagnosis: Automatic Spontaneous Speech Analysis},
  publisher = {Springer Berlin Heidelberg},
  year      = {2014},
  author    = {Lopez-de-Ipi{\~{n}}a, K. and Sol{\'e}-Casals, J. and Alonso, J. B. and Travieso, C. M. and Ecay, M. and Martinez-Lage, P.},
  editor    = {Nguyen, Ngoc Thanh and Kowalczyk, Ryszard and Fred, Ana and Joaquim, Filipe},
  address   = {Berlin, Heidelberg},
  abstract  = {Alzheimer’s disease (AD) is the most prevalent form of progressive degenerative dementia and it has a high socio-economic impact in Western countries therefore is one of the most active research areas today. Its diagnosis is sometimes made by excluding other dementias and definitive confirmation must be done through a post-mortem study of the brain tissue of the patient. The purpose of this paper is to contribute to the improvement of early diagnosis of AD and its degree of severity from an automatic analysis performed by non-invasive intelligent methods. The methods selected in this case are Automatic Spontaneous Speech Analysis (ASSA) and Emotional Temperature (ET) that have the great advantage of being non invasive low cost and without any side effects. The developed system obtains hopeful results for early diagnosis.},
  booktitle = {Transactions on Computational Collective Intelligence XVII},
  doi       = {10.1007/978-3-662-44994-3_14},
  isbn      = {978-3-662-44994-3},
  keywords  = {Alzheimer, emotional Speech, behaviomedics, diagnosis},
  url       = {http://dx.doi.org/10.1007/978-3-662-44994-3_14},
}

@Article{Moebes2008,
  author    = {Möbes, Janine and Joppich, Gregor and Stiebritz, Frank and Dengler, Reinhard and Schröder, Christine},
  title     = {Emotional speech in Parkinson's disease},
  journal   = {Movement Disorders},
  year      = {2008},
  volume    = {23},
  number    = {6},
  pages     = {824--829},
  abstract  = {Patients with Parkinson's disease (PD) tend to speak monotonously with minor modulation of pitch and intensity. The goal of this study was to find out whether these speech changes can be explained mainly by motor impairment, i.e. akinesia and rigidity of the articulatory apparatus, or whether alterations of emotional processing play an additional role. Sixteen patients with mild PD and 16 healthy controls (HC) were compared. Fundamental frequencies (pitch) and intensities (loudness) were determined as (1) maximal upper and lower values achieved in nonemotional speech (phonation capacity), (2) upper and lower values used when speaking “Anna” in emotional intonation (neutral, sad, happy) as requested (production task), or (3) when imitating a professional speaker (imitation task). Although groups did not significantly differ in their phonation capacity, patients showed a significantly smaller pitch and intensity range than HC in the production task. In the imitation task, however, ranges were again similar. These results suggest that alterations of emotional processing contribute to speech changes in PD, especially regarding emotional prosody, in addition to motor impairment. © 2008 Movement Disorder Society},
  doi       = {10.1002/mds.21940},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Moebes2008.pdf:PDF},
  issn      = {1531-8257},
  keywords  = {Parkinson's disease, vocal affective communication, emotional processing, speech expression, fundamental frequency, parkinson, emotional speech,},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  url       = {http://dx.doi.org/10.1002/mds.21940},
}

@Article{Singh2017,
  author    = {Rita Singh and Justin Baker and Luciana Pennant and Louis{-}Philippe Morency},
  title     = {Deducing the severity of psychiatric symptoms from the human voice},
  journal   = {CoRR},
  year      = {2017},
  volume    = {abs/1703.05344},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SinghBPM17},
  file      = {:Speech+Emotion/Singh2017.pdf:PDF},
  groups    = {Speech, To Read},
  keywords  = {emotional speech, depression, quantitative Evaluation, anxiety, severity},
  timestamp = {Wed, 07 Jun 2017 14:42:04 +0200},
  url       = {http://arxiv.org/abs/1703.05344},
}

@InProceedings{Valstar2014avec,
  author       = {Valstar, Michel and Schuller, Bj{\"o}rn and Smith, Kirsty and Almaev, Timur and Eyben, Florian and Krajewski, Jarek and Cowie, Roddy and Pantic, Maja},
  title        = {Avec 2014: 3d dimensional affect and depression recognition challenge},
  booktitle    = {Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge},
  year         = {2014},
  pages        = {3--10},
  organization = {ACM},
  file         = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Valstar2014avec.pdf:PDF},
  keywords     = {emotional Speech, depression, 3d, audio-visual emotion},
}

@InProceedings{Zhao2014,
  author    = {S. Zhao and F. Rudzicz and L. G. Carvalho and C. Marquez-Chin and S. Livingstone},
  title     = {Automatic detection of expressed emotion in Parkinson's Disease},
  booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2014},
  pages     = {4813-4817},
  month     = {May},
  abstract  = {Patients with Parkinsons Disease (PD) frequently exhibit deficits in the production of emotional speech. In this paper, we examine the classification of emotional speech in patients with PD and the classification of PD speech. Participants were recorded speaking short statements with different emotional prosody which were classified with three methods (naïve Bayes, random forests, and support vector machines) using 209 unique auditory features. Feature sets were reduced using simple statistical testing. We achieve accuracies of 65.5% and 73.33% on classifying between the emotions and between PD vs. control, respectively. These results may assist in the future development of automated early detection systems for diagnosing patients with PD.},
  doi       = {10.1109/ICASSP.2014.6854516},
  file      = {:Users/CarlaViegas/Dropbox/PhD Cloud/Publications/Emotion+Diseases/Zhao2014.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {Bayes methods;diseases;emotion recognition;medical signal processing;patient diagnosis;set theory;signal classification;speech processing;statistical testing;support vector machines;PD speech classification;automated early detection systems;automatic expressed emotion detection;emotional prosody;emotional speech classification;feature set reduction;naïve Bayes;patients-with-Parkinson's disease;random forests;statistical testing;support vector machines;unique auditory features;Accuracy;Mel frequency cepstral coefficient;Niobium;Parkinson's disease;Speech;Support vector machines;Parkinson's disease;acoustic features;classification;emotion, parkinson,},
}

@Article{ALM2015289,
  author   = {P.A. Alm},
  title    = {Is it Thinking and not Feeling that Influence Variability of Stuttering in Social Situations? About Stuttering and Social Cognition},
  journal  = {Procedia - Social and Behavioral Sciences},
  year     = {2015},
  volume   = {193},
  pages    = {289 - 290},
  note     = {10th Oxford Dysfluency Conference, ODC 2014, 17 - 20 July, 2014, Oxford, United kingdom},
  abstract = {In many cases of stuttering the severity of the symptoms tend to vary substantially in relation to the social situation, typically with less stuttering when talking all alone and more stuttering in socially demanding contexts. The factors underlying situational variability is of clinical importance. In theories of stuttering it has often been assumed that this variability is related to emotional reactions of anxiety and fear. However, the relation between emotions and stuttering is not clear. For example, observations of effects of strong fear in persons who stutter suggest that fear sometimes may facilitate speech fluency (Bloodstein & Ratner, 2008). Further, studies of effects of treatment in adults who stutter indicate that the anxiety for social situations may be successfully reduced without significant effect on the severity of the observable stuttering. It is here hypothesized that it is social cognition (thinking) and not social anxiety (emotion) that has the main interfering situational effect on stuttering. Social cognition involves thoughts about what one thinks of oneself, and what others may think or expect. For persons who are concerned about stuttering it is likely that social situations often involve thoughts about possible scenarios, including what other may think if they stutter and alternative plans how to act. Social cognition is a normal process, which does not need to be associated with social anxiety. Neuroscience research has shown that social cognition is especially related to processing in the medial prefrontal cortex (mPFC), which means the cortex region in the medial wall hidden between the two cerebral hemispheres in the most anterior part of the brain. This region is adjacent to regions which are crucial for the initiation of propositional speech, such as the anterior cingulate cortex and the supplementary motor area (SMA). Cortical regions in the brain may be roughly divided into two partly opposing networks: goal directed versus “reflecting”. It seems that activities which activate the goal-directed network, such as focused attention, tend to reduce the momentary severity of stuttering. One hypothesis about the cause of stuttering is that persons who stutter tend to have bilateral speech motor control. A consequence would be the need to synchronize both sides, via long pathways between the hemispheres. It is conceivable that such organization would be sensitive for interference, for example from processes related to social cognition in the medial frontal lobes. It is suggested that stuttering is a threshold phenomenon, meaning that fluent speech may be close to the neurophysiological threshold for disruption, but as long as the threshold is not passed, no obvious symptoms are shown. Thresholds create non-linear effects, which imply the possibility that significant interfering effect of social cognition may be shown also for normal social cognition, without strong social anxiety. It is important to emphasize that it is not suggested that social cognition is a factor underlying onset of stuttering, because data from preschool children who stutter do not provide support for elevated shyness or social anxiety at the time around onset.},
  doi      = {http://dx.doi.org/10.1016/j.sbspro.2015.03.327},
  issn     = {1877-0428},
  keywords = {Stuttering, Social cognition, Social anxiety, Neurophysiology},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877042815021205},
}

@Comment{jabref-meta: databaseType:bibtex;}
